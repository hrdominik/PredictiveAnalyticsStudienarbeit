{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Daily Financial News for 6000+ Stocks\r\n",
    "### Some Data Understanding an Preparation for Prediction of stockpricse by the Sentiment of Headlines\r\n",
    "@author DHR <br>\r\n",
    "@author BKN <br>\r\n",
    "used Data in this Notebook: 'https://www.kaggle.com/miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests'"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Requirements"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#immport basic DataScience Modules\r\n",
    "#!! for pip-modul-list to install @see requirements.txt\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#import basic topic modelling\r\n",
    "import re\r\n",
    "from gensim.utils import simple_preprocess\r\n",
    "import gensim.corpora as corpora\r\n",
    "from pprint import pprint\r\n",
    "import gensim\r\n",
    "\r\n",
    "import pyLDAvis\r\n",
    "import pyLDAvis.gensim_models\r\n",
    "import pickle "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#import basic visualization\r\n",
    "from wordcloud import WordCloud, STOPWORDS\r\n",
    "import nltk #tokenization\r\n",
    "#nltk.download('punkt')\r\n",
    "nltk.download('stopwords')\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "import random"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kaste\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#imports for generell Cleaning and Merging\r\n",
    "import datetime"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "#imports headline cleaning\r\n",
    "import nltk\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "import re\r\n",
    "import spacy\r\n",
    "from nltk.corpus import sentiwordnet as swn\r\n",
    "from nltk.corpus import wordnet as wn\r\n",
    "nltk.download('wordnet')\r\n",
    "nltk.download('averaged_perceptron_tagger')\r\n",
    "\r\n",
    "from sentistrength import PySentiStr\r\n",
    "# from stanfordcorenlp import StanfordCoreNLP\r\n",
    "# nlp = StanfordCoreNLP(r'G:\\JavaLibraries\\stanford-corenlp-full-2018-02-27')\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kaste\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kaste\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#import basic api requirements\r\n",
    "from polygon import RESTClient\r\n",
    "import time\r\n",
    "from dotenv import load_dotenv\r\n",
    "load_dotenv()\r\n",
    "\r\n",
    "def unique(l):\r\n",
    "    ## list of only the unique values from a given list\r\n",
    "    x = np.array(l)\r\n",
    "    return np.unique(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#Load Data\r\n",
    "data = pd.read_csv('../data/raw_analyst_ratings.csv')\r\n",
    "#colums: id,headline,url,publisher,date,stock"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Understanding\r\n",
    "\r\n",
    "Without Data Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#first Look\r\n",
    "print(data.columns)\r\n",
    "data.sample(7)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['Unnamed: 0', 'headline', 'url', 'publisher', 'date', 'stock'], dtype='object')\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Unnamed: 0                                           headline  \\\n",
       "1357309     1363727            10 Biggest Mid-Day Losers For Wednesday   \n",
       "162164       163239            Earnings Scheduled For January 28, 2013   \n",
       "1115275     1120770  A Climb In The UltraShort Crude Oil ETF (NYSE:...   \n",
       "991291       996253  BZ NOTE: National Beverage Recently Unveiled (...   \n",
       "563262       566247  UPDATE: GenVec Says Co. Ended '15 with ~$8.7M ...   \n",
       "1002900     1007899  Summit Research's Srini Sundararajan on Philip...   \n",
       "350329       352305  Stifel Nicolaus Maintains Buy on Dean Foods Co...   \n",
       "\n",
       "                                                       url          publisher  \\\n",
       "1357309  https://www.benzinga.com/news/17/07/9769448/10...         Lisa Levin   \n",
       "162164   https://www.benzinga.com/news/earnings/13/01/3...      Monica Gerson   \n",
       "1115275  https://www.benzinga.com/10/08/427851/a-climb-...  David Bettencourt   \n",
       "991291   https://www.benzinga.com/movers/16/06/8135221/...      Paul Quintaro   \n",
       "563262   https://www.benzinga.com/news/16/01/6154484/up...      Paul Quintaro   \n",
       "1002900  https://www.benzinga.com/analyst-ratings/analy...      Paul Quintaro   \n",
       "350329   https://www.benzinga.com/news/13/11/4076232/st...         Juan Lopez   \n",
       "\n",
       "                        date stock  \n",
       "1357309  2017-07-12 00:00:00  WMLP  \n",
       "162164   2013-01-28 00:00:00  BIIB  \n",
       "1115275  2010-08-13 00:00:00   SCO  \n",
       "991291   2016-06-21 00:00:00   PEP  \n",
       "563262   2016-01-20 00:00:00  GNVC  \n",
       "1002900  2016-01-26 00:00:00   PHG  \n",
       "350329   2013-11-13 00:00:00    DF  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>headline</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>date</th>\n",
       "      <th>stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1357309</th>\n",
       "      <td>1363727</td>\n",
       "      <td>10 Biggest Mid-Day Losers For Wednesday</td>\n",
       "      <td>https://www.benzinga.com/news/17/07/9769448/10...</td>\n",
       "      <td>Lisa Levin</td>\n",
       "      <td>2017-07-12 00:00:00</td>\n",
       "      <td>WMLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162164</th>\n",
       "      <td>163239</td>\n",
       "      <td>Earnings Scheduled For January 28, 2013</td>\n",
       "      <td>https://www.benzinga.com/news/earnings/13/01/3...</td>\n",
       "      <td>Monica Gerson</td>\n",
       "      <td>2013-01-28 00:00:00</td>\n",
       "      <td>BIIB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115275</th>\n",
       "      <td>1120770</td>\n",
       "      <td>A Climb In The UltraShort Crude Oil ETF (NYSE:...</td>\n",
       "      <td>https://www.benzinga.com/10/08/427851/a-climb-...</td>\n",
       "      <td>David Bettencourt</td>\n",
       "      <td>2010-08-13 00:00:00</td>\n",
       "      <td>SCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991291</th>\n",
       "      <td>996253</td>\n",
       "      <td>BZ NOTE: National Beverage Recently Unveiled (...</td>\n",
       "      <td>https://www.benzinga.com/movers/16/06/8135221/...</td>\n",
       "      <td>Paul Quintaro</td>\n",
       "      <td>2016-06-21 00:00:00</td>\n",
       "      <td>PEP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563262</th>\n",
       "      <td>566247</td>\n",
       "      <td>UPDATE: GenVec Says Co. Ended '15 with ~$8.7M ...</td>\n",
       "      <td>https://www.benzinga.com/news/16/01/6154484/up...</td>\n",
       "      <td>Paul Quintaro</td>\n",
       "      <td>2016-01-20 00:00:00</td>\n",
       "      <td>GNVC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002900</th>\n",
       "      <td>1007899</td>\n",
       "      <td>Summit Research's Srini Sundararajan on Philip...</td>\n",
       "      <td>https://www.benzinga.com/analyst-ratings/analy...</td>\n",
       "      <td>Paul Quintaro</td>\n",
       "      <td>2016-01-26 00:00:00</td>\n",
       "      <td>PHG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350329</th>\n",
       "      <td>352305</td>\n",
       "      <td>Stifel Nicolaus Maintains Buy on Dean Foods Co...</td>\n",
       "      <td>https://www.benzinga.com/news/13/11/4076232/st...</td>\n",
       "      <td>Juan Lopez</td>\n",
       "      <td>2013-11-13 00:00:00</td>\n",
       "      <td>DF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#statistics\r\n",
    "print(\"observations: {}, features: {} \\n\".format(data.shape[0], data.shape[1]))\r\n",
    "print(\"unique headlines: {}, unique stocks: {} \\n\".format(len(data.headline.unique()), len(data.stock.unique())))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stock = data.groupby(\"stock\")\r\n",
    "stock.sample()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stock.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Visualization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# BoxPlot of Stocks\r\n",
    "data['stock'].value_counts().plot.box(vert=False, figsize=(30,10))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# BocPlot of Date\r\n",
    "data['date'].value_counts().plot.box(vert=False, figsize=(30,10))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Number of Headlines by Stock\r\n",
    "plt.figure(figsize=(30,10))\r\n",
    "ax = stock.size().sort_values(ascending=False)[0:6204].plot.bar()\r\n",
    "plt.xticks(rotation=50)\r\n",
    "plt.xlabel(\"Stock\")\r\n",
    "plt.ylabel(\"Number of Headlines\")\r\n",
    "#only display every n Label\r\n",
    "n = 50\r\n",
    "for i, t in enumerate(ax.get_xticklabels()):\r\n",
    "    if (i % n) != 0:\r\n",
    "        t.set_visible(False)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Number of Headlines by Date\r\n",
    "date = data.groupby(\"date\")\r\n",
    "\r\n",
    "plt.figure(figsize=(30,10))\r\n",
    "ax = date.size().sort_values(ascending=False)[0:1000].plot.bar()\r\n",
    "plt.xticks(rotation=50)\r\n",
    "plt.xlabel(\"Date\")\r\n",
    "plt.ylabel(\"Number of Headlines\")\r\n",
    "\r\n",
    "#only display every n Label\r\n",
    "n = 50\r\n",
    "for i, t in enumerate(ax.get_xticklabels()):\r\n",
    "    if (i % n) != 0:\r\n",
    "        t.set_visible(False)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "date_withYear = data.assign(year = lambda dataframe: dataframe['date'].map(lambda date: date[0:4]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Der Datensatz erstreckt sich über einen Datumsspanne vom \"+ data['date'].min()[0:10] +\" bis zum \"+ data['date'].max()[0:10] +\"\\nDabei besteht der Datensatz aus \"+ str(data.headline.count()) + \" Einträgen\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Number of Date by Year\r\n",
    "year = date_withYear.groupby(\"year\")\r\n",
    "\r\n",
    "plt.figure(figsize=(30,10))\r\n",
    "year.size().plot.bar()\r\n",
    "plt.xticks(rotation=50)\r\n",
    "plt.xlabel(\"Year\")\r\n",
    "plt.ylabel(\"Number of Healines\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import statistics\r\n",
    "\r\n",
    "print(statistics.mean(year.size()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Termdocument Matrix\r\n",
    "\r\n",
    "# all headlines as String in List\r\n",
    "docs = []\r\n",
    "for headline in data.headline:\r\n",
    "    docs.append(headline)\r\n",
    "docs = docs[0:1000]\r\n",
    "\r\n",
    "vec = CountVectorizer()\r\n",
    "X = vec.fit_transform(docs)\r\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\r\n",
    "print(df)\r\n",
    "\r\n",
    "# !! TDM just useful for stemmend and removed Stopwords dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Wordclouds before Data Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Wordcloud for all Headlines\r\n",
    "\r\n",
    "#combine all headlines to one text\r\n",
    "text = \" \".join(headline for headline in data.headline)\r\n",
    "maxWords = 50\r\n",
    "#wordcloud\r\n",
    "#!! No Stopword removal\r\n",
    "# stopwords = STOPWORDS\r\n",
    "# stopwords.update([\"Benzinga\", \"Stocks\", \"vs\", \"Est\", \"EPS\"])\r\n",
    "wordcloud = WordCloud(max_words=maxWords).generate(text)\r\n",
    "\r\n",
    "#plot\r\n",
    "print(\"WorldCloud over all Stocks, Top \" + str(maxWords) + \" Words:\")\r\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\r\n",
    "plt.axis(\"off\")\r\n",
    "plt.show()\r\n",
    "\r\n",
    "#save\r\n",
    "wordcloud.to_file(\"./results/wordcloud.png\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Wordcloud for one Stock\r\n",
    "\r\n",
    "#combine all headlines of one Stock\r\n",
    "stockFilter = random.choice(unique(data['stock']))\r\n",
    "maxWords = 50\r\n",
    "text_SpecificStock = \" \".join(headline for headline in data[data[\"stock\"]==stockFilter].headline)\r\n",
    "\r\n",
    "#wordcloud\r\n",
    "#!! No Stopword removal\r\n",
    "wordcloud = WordCloud(max_words=maxWords).generate(text_SpecificStock)\r\n",
    "\r\n",
    "#plot\r\n",
    "print(\"WorldCloud for Random Stock: \" + stockFilter + \", Top \" + str(maxWords) + \" Words:\")\r\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\r\n",
    "plt.axis(\"off\")\r\n",
    "plt.show()\r\n",
    "\r\n",
    "#save\r\n",
    "wordcloud.to_file(\"./results/wordcloud_stock-\"+ stockFilter +\".png\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Top n-Words\r\n",
    "n = 20\r\n",
    "\r\n",
    "wordFreq = WordCloud().process_text(text)\r\n",
    "wordFreq = dict(sorted(wordFreq.items(), key=lambda item: item[1], reverse=True)[:n])\r\n",
    "\r\n",
    "plt.bar(range(len(wordFreq)), list(wordFreq.values()), align='center')\r\n",
    "plt.xticks(range(len(wordFreq)), list(wordFreq.keys()), rotation=50)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Topic Modelling before Data Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Topic Modelling with topicCount Topics\r\n",
    "topicCount = 10\r\n",
    "\r\n",
    "## Necessary Text Cleaning\r\n",
    "# Remove punctuation\r\n",
    "# data['headline_processed'] = \\\r\n",
    "# data['headline'].map(lambda x: re.sub('[,\\.!?]', '', x))\r\n",
    "# Convert the titles to lowercase\r\n",
    "# data['headline_processed'] = \\\r\n",
    "# data['headline_processed'].map(lambda x: x.lower())\r\n",
    "\r\n",
    "# stop_words = stopwords.words('english')\r\n",
    "# stop_words.extend(['bezinga', 'stock'])\r\n",
    "\r\n",
    "## Necessary Tokenzisation of Sentences to Words\r\n",
    "def sent_to_words(sentences):\r\n",
    "    for sentence in sentences:\r\n",
    "        # deacc=True removes punctuations\r\n",
    "        yield(simple_preprocess(str(sentence), deacc=True))\r\n",
    "# def remove_stopwords(texts):\r\n",
    "#     return [[word for word in simple_preprocess(str(doc)) \r\n",
    "#              if word not in stop_words] for doc in texts]\r\n",
    "\r\n",
    "\r\n",
    "headline = data.headline.values.tolist() #use headline_processed instead of headline for a little bit of DataCleaning\r\n",
    "headline_words = list(sent_to_words(headline))\r\n",
    "# remove stop words\r\n",
    "# headline_words = remove_stopwords(headline_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create Dictionary\r\n",
    "id2word = corpora.Dictionary(headline_words)\r\n",
    "# Create Corpus\r\n",
    "texts = headline_words\r\n",
    "# Term Document Frequency\r\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The Real Topic Modelling\r\n",
    "# Build LDA model\r\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\r\n",
    "                                       id2word=id2word,\r\n",
    "                                       num_topics=topicCount)\r\n",
    "# Print the Keyword in the topics\r\n",
    "pprint(lda_model.print_topics())\r\n",
    "doc_lda = lda_model[corpus]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualization of the Topic Modelling\r\n",
    "\r\n",
    "pyLDAvis.enable_notebook()\r\n",
    "LDAvis_data_filepath = os.path.join('./app/results_ldavis_prepared_'+str(topicCount))\r\n",
    "# # this is a bit time consuming - make the if statement True\r\n",
    "# # if you want to execute visualization prep yourself\r\n",
    "if 1 == 1:\r\n",
    "    LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\r\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\r\n",
    "        pickle.dump(LDAvis_prepared, f)\r\n",
    "# load the pre-prepared pyLDAvis data from disk\r\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\r\n",
    "    LDAvis_prepared = pickle.load(f)\r\n",
    "pyLDAvis.save_html(LDAvis_prepared, './app/results_ldavis_prepared_'+ str(topicCount) +'.html')\r\n",
    "LDAvis_prepared"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Quality"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# count all null values in the DataFrame\r\n",
    "print(\"Anzahl von null Werten im gesamten DataFrame: \"+str(data.isna().sum().sum()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Anzahl von null Werten im gesamten DataFrame: 0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# count all dates without time, no need to format in isoFormat\r\n",
    "print(\"Anzahl Zeitstempel ohne konkrete Uhrzeit: \"+ str(data['date'].str.count('00:00:00').sum()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Anzahl Zeitstempel ohne konkrete Uhrzeit: 1351341\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# count duplicated\r\n",
    "print(\"Anzahl Doppelten Einträgen: \"+ str(data.duplicated().sum()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Anzahl Doppelten Einträgen: 0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Date Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# format datetime to date\r\n",
    "data = data.assign(\r\n",
    "    date = lambda dataframe: dataframe['date'].map(lambda date: date[0:10])\r\n",
    ")\r\n",
    "\r\n",
    "# delete all rows older than 2years, because of API and Performance\r\n",
    "data = data.drop(data[data.date < '2019-08-21'].index)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# clean the Stocks\r\n",
    "# Remove Rows with Stocks that occur less than occ times\r\n",
    "occ = 0\r\n",
    "data = data[data.groupby('stock').stock.transform(len) > occ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "print(\"Der Datensatz erstreckt sich nun über einen Datumsspanne vom \"+ data['date'].min() +\" bis zum \"+ data['date'].max() +\"\\nDabei besteht der Datensatz jetzt aus \"+ str(data.headline.count()) + \" Einträgen\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Der Datensatz erstreckt sich nun über einen Datumsspanne vom 2019-08-21 bis zum 2020-06-11\n",
      "Dabei besteht der Datensatz jetzt aus 164698 Einträgen\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Headline Cleaning & Pre-Processing\r\n",
    "Hier wird nochmal speziell jede einzelene Headline den typischen pre-processing Schritten unterzogen"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "#daten nochmal neu laden falls reset\r\n",
    "data = pd.read_csv('../data/raw_analyst_ratings.csv')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "#einen ausschnitt zum testen verwenden\r\n",
    "data = data[:1000]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "data.sample()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    Unnamed: 0                                           headline  \\\n",
       "14          14  Agilent Technologies Q2 Adj. EPS $0.71 Beats $...   \n",
       "\n",
       "                                                  url          publisher  \\\n",
       "14  https://www.benzinga.com/news/earnings/20/05/1...  Benzinga Newsdesk   \n",
       "\n",
       "                   date stock  \n",
       "14  2020-05-21 00:00:00     A  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>headline</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>date</th>\n",
       "      <th>stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>Agilent Technologies Q2 Adj. EPS $0.71 Beats $...</td>\n",
       "      <td>https://www.benzinga.com/news/earnings/20/05/1...</td>\n",
       "      <td>Benzinga Newsdesk</td>\n",
       "      <td>2020-05-21 00:00:00</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "#Headlines in lowercase konvertieren\r\n",
    "\r\n",
    "data['headline_cleaned'] = data['headline'].apply(lambda headline: headline.lower())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "data.sample(10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     Unnamed: 0                                           headline  \\\n",
       "705         705  UBS Initiates Agilent Technologies at Buy, $43 PT   \n",
       "483         483  George A. Scangos, Ph.D., of Biogen Idec to Jo...   \n",
       "67           67  Wells Fargo Initiates Coverage On Agilent Tech...   \n",
       "697         697  Bank of America Upgrades Agilent Technologies ...   \n",
       "377         377  Bruker Corp Has 'Great Story,' But It's Not Ti...   \n",
       "155         155  Agilent Technologies Option Alert: Mar 15 $75 ...   \n",
       "559         559  ISI Group Maintains Buy on Agilent Technologie...   \n",
       "353         353                            Benzinga's Top Upgrades   \n",
       "360         360  Goldman Sachs Upgrades Agilent Technologies to...   \n",
       "882         889  Blackstone Group And Glimcher Realty Trust For...   \n",
       "\n",
       "                                                   url          publisher  \\\n",
       "705  https://www.benzinga.com/analyst-ratings/price...          Joe Young   \n",
       "483  https://www.benzinga.com/news/14/03/4403849/ge...       Eddie Staley   \n",
       "67   https://www.benzinga.com/news/20/01/15090063/w...  Benzinga_Newsdesk   \n",
       "697  https://www.benzinga.com/analyst-ratings/upgra...          Joe Young   \n",
       "377  https://www.benzinga.com/analyst-ratings/analy...        Jim Swanson   \n",
       "155  https://www.benzinga.com/markets/options/19/01...      Charles Gross   \n",
       "559  https://www.benzinga.com/news/13/05/3591128/is...         Juan Lopez   \n",
       "353  https://www.benzinga.com/analyst-ratings/upgra...      Monica Gerson   \n",
       "360  https://www.benzinga.com/news/15/12/6028742/go...      Paul Quintaro   \n",
       "882  https://www.benzinga.com/markets/company-news/...     Benzinga Staff   \n",
       "\n",
       "                    date stock  \\\n",
       "705  2011-10-24 00:00:00     A   \n",
       "483  2014-03-19 00:00:00     A   \n",
       "67   2020-01-08 00:00:00     A   \n",
       "697  2011-11-15 00:00:00     A   \n",
       "377  2015-09-15 00:00:00     A   \n",
       "155  2019-01-29 00:00:00     A   \n",
       "559  2013-05-15 00:00:00     A   \n",
       "353  2016-01-07 00:00:00     A   \n",
       "360  2015-12-08 00:00:00     A   \n",
       "882  2009-11-05 00:00:00     A   \n",
       "\n",
       "                                      headline_cleaned  \n",
       "705  ubs initiates agilent technologies at buy, $43 pt  \n",
       "483  george a. scangos, ph.d., of biogen idec to jo...  \n",
       "67   wells fargo initiates coverage on agilent tech...  \n",
       "697  bank of america upgrades agilent technologies ...  \n",
       "377  bruker corp has 'great story,' but it's not ti...  \n",
       "155  agilent technologies option alert: mar 15 $75 ...  \n",
       "559  isi group maintains buy on agilent technologie...  \n",
       "353                            benzinga's top upgrades  \n",
       "360  goldman sachs upgrades agilent technologies to...  \n",
       "882  blackstone group and glimcher realty trust for...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>headline</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>date</th>\n",
       "      <th>stock</th>\n",
       "      <th>headline_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>705</td>\n",
       "      <td>UBS Initiates Agilent Technologies at Buy, $43 PT</td>\n",
       "      <td>https://www.benzinga.com/analyst-ratings/price...</td>\n",
       "      <td>Joe Young</td>\n",
       "      <td>2011-10-24 00:00:00</td>\n",
       "      <td>A</td>\n",
       "      <td>ubs initiates agilent technologies at buy, $43 pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>483</td>\n",
       "      <td>George A. Scangos, Ph.D., of Biogen Idec to Jo...</td>\n",
       "      <td>https://www.benzinga.com/news/14/03/4403849/ge...</td>\n",
       "      <td>Eddie Staley</td>\n",
       "      <td>2014-03-19 00:00:00</td>\n",
       "      <td>A</td>\n",
       "      <td>george a. scangos, ph.d., of biogen idec to jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>67</td>\n",
       "      <td>Wells Fargo Initiates Coverage On Agilent Tech...</td>\n",
       "      <td>https://www.benzinga.com/news/20/01/15090063/w...</td>\n",
       "      <td>Benzinga_Newsdesk</td>\n",
       "      <td>2020-01-08 00:00:00</td>\n",
       "      <td>A</td>\n",
       "      <td>wells fargo initiates coverage on agilent tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>697</td>\n",
       "      <td>Bank of America Upgrades Agilent Technologies ...</td>\n",
       "      <td>https://www.benzinga.com/analyst-ratings/upgra...</td>\n",
       "      <td>Joe Young</td>\n",
       "      <td>2011-11-15 00:00:00</td>\n",
       "      <td>A</td>\n",
       "      <td>bank of america upgrades agilent technologies ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>377</td>\n",
       "      <td>Bruker Corp Has 'Great Story,' But It's Not Ti...</td>\n",
       "      <td>https://www.benzinga.com/analyst-ratings/analy...</td>\n",
       "      <td>Jim Swanson</td>\n",
       "      <td>2015-09-15 00:00:00</td>\n",
       "      <td>A</td>\n",
       "      <td>bruker corp has 'great story,' but it's not ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>155</td>\n",
       "      <td>Agilent Technologies Option Alert: Mar 15 $75 ...</td>\n",
       "      <td>https://www.benzinga.com/markets/options/19/01...</td>\n",
       "      <td>Charles Gross</td>\n",
       "      <td>2019-01-29 00:00:00</td>\n",
       "      <td>A</td>\n",
       "      <td>agilent technologies option alert: mar 15 $75 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>559</td>\n",
       "      <td>ISI Group Maintains Buy on Agilent Technologie...</td>\n",
       "      <td>https://www.benzinga.com/news/13/05/3591128/is...</td>\n",
       "      <td>Juan Lopez</td>\n",
       "      <td>2013-05-15 00:00:00</td>\n",
       "      <td>A</td>\n",
       "      <td>isi group maintains buy on agilent technologie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>353</td>\n",
       "      <td>Benzinga's Top Upgrades</td>\n",
       "      <td>https://www.benzinga.com/analyst-ratings/upgra...</td>\n",
       "      <td>Monica Gerson</td>\n",
       "      <td>2016-01-07 00:00:00</td>\n",
       "      <td>A</td>\n",
       "      <td>benzinga's top upgrades</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>360</td>\n",
       "      <td>Goldman Sachs Upgrades Agilent Technologies to...</td>\n",
       "      <td>https://www.benzinga.com/news/15/12/6028742/go...</td>\n",
       "      <td>Paul Quintaro</td>\n",
       "      <td>2015-12-08 00:00:00</td>\n",
       "      <td>A</td>\n",
       "      <td>goldman sachs upgrades agilent technologies to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>889</td>\n",
       "      <td>Blackstone Group And Glimcher Realty Trust For...</td>\n",
       "      <td>https://www.benzinga.com/markets/company-news/...</td>\n",
       "      <td>Benzinga Staff</td>\n",
       "      <td>2009-11-05 00:00:00</td>\n",
       "      <td>A</td>\n",
       "      <td>blackstone group and glimcher realty trust for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "#POS Tagging\r\n",
    "#Beim Part Of Speech Tagging wird geprüft, ob es sich um ein Nomen, Adjevtiv etc. handelt. Das unterstützt das Lammatisieren\r\n",
    "\r\n",
    "def tokenize_post(headline):\r\n",
    "    hl_tokenz = word_tokenize(headline)\r\n",
    "    hl_post = nltk.pos_tag(hl_tokenz)\r\n",
    "    hl_post_result = []\r\n",
    "    for word in hl_post:\r\n",
    "        if word[1].startswith('NN'):\r\n",
    "            hl_post_result.append([word[0], 'n'])\r\n",
    "        elif word[1].startswith('JJ'):\r\n",
    "            hl_post_result.append([word[0], 'a'])\r\n",
    "        elif word[1].startswith('V'):\r\n",
    "            hl_post_result.append([word[0], 'v'])\r\n",
    "        elif word[1].startswith('R'):\r\n",
    "            hl_post_result.append([word[0], 'r'])\r\n",
    "        else:\r\n",
    "            hl_post_result.append([word[0], ''])\r\n",
    "    return hl_post_result\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "#Tokenization und POS-Tagging durchführen\r\n",
    "data['headline_cleaned'] = data['headline'].apply(tokenize_post)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "CC coordinating conjunction\r\n",
    "CD cardinal digit\r\n",
    "DT determiner\r\n",
    "EX existential there (like: “there is” … think of it like “there exists”)\r\n",
    "FW foreign word\r\n",
    "IN preposition/subordinating conjunction\r\n",
    "JJ adjective ‘big’\r\n",
    "JJR adjective, comparative ‘bigger’\r\n",
    "JJS adjective, superlative ‘biggest’\r\n",
    "LS list marker 1)\r\n",
    "MD modal could, will\r\n",
    "NN noun, singular ‘desk’\r\n",
    "NNS noun plural ‘desks’\r\n",
    "NNP proper noun, singular ‘Harrison’\r\n",
    "NNPS proper noun, plural ‘Americans’\r\n",
    "PDT predeterminer ‘all the kids’\r\n",
    "POS possessive ending parent’s\r\n",
    "PRP personal pronoun I, he, she\r\n",
    "PRP$ possessive pronoun my, his, hers\r\n",
    "RB adverb very, silently,\r\n",
    "RBR adverb, comparative better\r\n",
    "RBS adverb, superlative best\r\n",
    "RP particle give up\r\n",
    "TO, to go ‘to’ the store.\r\n",
    "UH interjection, errrrrrrrm\r\n",
    "VB verb, base form take\r\n",
    "VBD verb, past tense took\r\n",
    "VBG verb, gerund/present participle taking\r\n",
    "VBN verb, past participle taken\r\n",
    "VBP verb, sing. present, non-3d take\r\n",
    "VBZ verb, 3rd person sing. present takes\r\n",
    "WDT wh-determiner which\r\n",
    "WP wh-pronoun who, what\r\n",
    "WP$ possessive wh-pronoun whose\r\n",
    "WRB wh-abverb where, when"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "#funktionen für  Stopword-removal\r\n",
    "    \r\n",
    "#Stopwords aus dem NLTK Modul nehmen und entfernen\r\n",
    "def remove_stopwords(hl_post_tok_lower):\r\n",
    "    stop_words = set(stopwords.words('english')) \r\n",
    "\r\n",
    "    #hier könnte man noch ein paar eigene Stopwords hinzufügen\r\n",
    "    # stop_words.add('')\r\n",
    "\r\n",
    "    filtered_sentence = [word for word in hl_post_tok_lower if not word[0] in stop_words]\r\n",
    "    return filtered_sentence"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "#jetzt das stopword removal durchführen\r\n",
    "data['headline_cleaned'] = data['headline_cleaned'].apply(remove_stopwords)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "#Hier eine Lemmatisierungsfunktion mit dem WortNetLemmatizer\r\n",
    "lemmatizer = WordNetLemmatizer()\r\n",
    "\r\n",
    "#this headline is pos-tagged, tokenzied, lower, and stopwords-removed\r\n",
    "def lemmatize(headline):\r\n",
    "    lemmatized_output = []\r\n",
    "    for word in headline:\r\n",
    "        if word[1] == '':\r\n",
    "            lemmatized_output.append((lemmatizer.lemmatize(word[0]), word[1]))\r\n",
    "        else:\r\n",
    "            lemmatized_output.append((lemmatizer.lemmatize(word[0], pos=word[1]), word[1]))\r\n",
    "\r\n",
    "    #Alle Wörter mit weniger als zwei Zeichen weg\r\n",
    "    lemmatized_output = [word for word in lemmatized_output if len(word[0]) > 2]\r\n",
    "\r\n",
    "    #Alle Zahlen entfernen\r\n",
    "    lemmatized_output = [word for word in lemmatized_output if not word[0].isnumeric()]\r\n",
    "\r\n",
    "    return lemmatized_output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "#lemmatization durchführen\r\n",
    "\r\n",
    "#nächste Zeile um daten zu laden\r\n",
    "# data_important = pd.read_csv('../data/zwischenergebnis_stopwords_tokenized.csv')\r\n",
    "data['headline_cleaned'] = data['headline_cleaned'].apply(lemmatize)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "#Mit ein Synset ist eine Liste von den gebräuchlisten Synonymen für ein Wort. zu jedem Wort nehmen wir das gebräuchliste Synonym, um möglichst viele Senitment Werte zu bekommen. \r\n",
    "def getSynset(headline):\r\n",
    "    synset_output = []\r\n",
    "    for word in headline:\r\n",
    "        synsets = wn.synsets(word[0], pos=word[1])\r\n",
    "        if len(synsets)>0:\r\n",
    "            synset_output.append((synsets[0].name()))\r\n",
    "        # wort entfernen wenn kein senitment verfügbar\r\n",
    "        # else:\r\n",
    "        #     synset_output.append(word[0]+ \".\" +word[1]+\".01\")\r\n",
    "    return synset_output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "#Synset berechnen\r\n",
    "data['headline_cleaned'] = data['headline_cleaned'].apply(getSynset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "#calculate sentiment\r\n",
    "nltk.download('sentiwordnet')\r\n",
    "senti_pos_score = []\r\n",
    "senti_neg_score = []\r\n",
    "\r\n",
    "senti_score = []\r\n",
    "# senti_obj_score = []\r\n",
    "\r\n",
    "#wir sollten ein eignens Dictionary bauen, dass spezielle Wörter für Aktiennews beinhaltet, wie hit, raise etc. und\r\n",
    "def getSentiment(headline):\r\n",
    "    senti_pos = 0\r\n",
    "    senti_neg = 0\r\n",
    "    # senti_obj = 0\r\n",
    "    for word in headline:\r\n",
    "        #hier erst prüfen, ob das wort in unserem eigenen dictionary ist\r\n",
    "        swn_synset = swn.senti_synset(word)\r\n",
    "        senti_pos += swn_synset.pos_score()\r\n",
    "        senti_neg += swn_synset.neg_score()\r\n",
    "\r\n",
    "    senti_pos_score.append(senti_pos)\r\n",
    "    senti_neg_score.append(senti_neg)\r\n",
    "\r\n",
    "    senti_score.append(senti_pos - senti_neg)\r\n",
    "    # senti_obj_score.append(swn_synset.obj_score())\r\n",
    "    return headline"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\kaste\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "# calculate Sentiment\r\n",
    "data['headline_cleaned'] = data['headline_cleaned'].apply(getSentiment)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "# add Sentiment to DataFrame\r\n",
    "data['senti_pos_score'] = senti_pos_score\r\n",
    "data['senti_neg_score'] = senti_neg_score\r\n",
    "\r\n",
    "data['senti_score'] = senti_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "data.sample(7)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     Unnamed: 0                                           headline  \\\n",
       "106         106  Agilent Technologies shares are trading higher...   \n",
       "98           98  Shares of several healthcare companies are tra...   \n",
       "874         881  The Health Care Reform Bill: Welcome To The Bi...   \n",
       "698         698  US Stock Futures Down Ahead Of Earnings, Econo...   \n",
       "710         710  Financials, Industrials, and Basic Materials S...   \n",
       "548         548    Earnings Expectations For The Week Of August 12   \n",
       "243         243  Agilent Files Civil Lawsuit against Shanghai E...   \n",
       "\n",
       "                                                   url          publisher  \\\n",
       "106  https://www.benzinga.com/markets/wiim/19/08/14...  Benzinga Newsdesk   \n",
       "98   https://www.benzinga.com/markets/wiim/19/09/14...  Benzinga Newsdesk   \n",
       "874  https://www.benzinga.com/189402/the-health-car...     Michael Snyder   \n",
       "698  https://www.benzinga.com/news/11/11/2127931/us...         Lisa Levin   \n",
       "710  https://www.benzinga.com/trading-ideas/long-id...       Benjamin Lee   \n",
       "548  https://www.benzinga.com/news/earnings/13/08/3...         Nelson Hem   \n",
       "243  https://www.benzinga.com/news/17/08/9964556/ag...      Charles Gross   \n",
       "\n",
       "                    date stock  \\\n",
       "106  2019-08-14 00:00:00     A   \n",
       "98   2019-09-05 00:00:00     A   \n",
       "874  2010-03-11 00:00:00     A   \n",
       "698  2011-11-15 00:00:00     A   \n",
       "710  2011-09-08 00:00:00     A   \n",
       "548  2013-08-11 00:00:00     A   \n",
       "243  2017-08-23 00:00:00     A   \n",
       "\n",
       "                                      headline_cleaned  senti_pos_score  \\\n",
       "106  [technology.n.01, share.n.01, trade.v.01, high...            0.125   \n",
       "98   [share.n.01, several.s.01, healthcare.n.01, co...            0.500   \n",
       "874  [health.n.01, care.n.01, reform.n.01, bill.n.0...            1.250   \n",
       "698  [stock.n.01, future.n.01, down.n.01, net_incom...            0.125   \n",
       "710  [basic.n.01, material.n.01, slide.n.01, stoppi...            0.000   \n",
       "548  [net_income.n.01, expectation.n.01, week.n.01,...            0.000   \n",
       "243  [file.n.01, lawsuit.n.01, shanghai.n.01, techn...            0.000   \n",
       "\n",
       "     senti_neg_score  senti_score  \n",
       "106            0.500       -0.375  \n",
       "98             0.625       -0.125  \n",
       "874            0.000        1.250  \n",
       "698            0.000        0.125  \n",
       "710            0.000        0.000  \n",
       "548            0.000        0.000  \n",
       "243            0.000        0.000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>headline</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>date</th>\n",
       "      <th>stock</th>\n",
       "      <th>headline_cleaned</th>\n",
       "      <th>senti_pos_score</th>\n",
       "      <th>senti_neg_score</th>\n",
       "      <th>senti_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>106</td>\n",
       "      <td>Agilent Technologies shares are trading higher...</td>\n",
       "      <td>https://www.benzinga.com/markets/wiim/19/08/14...</td>\n",
       "      <td>Benzinga Newsdesk</td>\n",
       "      <td>2019-08-14 00:00:00</td>\n",
       "      <td>A</td>\n",
       "      <td>[technology.n.01, share.n.01, trade.v.01, high...</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>Shares of several healthcare companies are tra...</td>\n",
       "      <td>https://www.benzinga.com/markets/wiim/19/09/14...</td>\n",
       "      <td>Benzinga Newsdesk</td>\n",
       "      <td>2019-09-05 00:00:00</td>\n",
       "      <td>A</td>\n",
       "      <td>[share.n.01, several.s.01, healthcare.n.01, co...</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.625</td>\n",
       "      <td>-0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>881</td>\n",
       "      <td>The Health Care Reform Bill: Welcome To The Bi...</td>\n",
       "      <td>https://www.benzinga.com/189402/the-health-car...</td>\n",
       "      <td>Michael Snyder</td>\n",
       "      <td>2010-03-11 00:00:00</td>\n",
       "      <td>A</td>\n",
       "      <td>[health.n.01, care.n.01, reform.n.01, bill.n.0...</td>\n",
       "      <td>1.250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>698</td>\n",
       "      <td>US Stock Futures Down Ahead Of Earnings, Econo...</td>\n",
       "      <td>https://www.benzinga.com/news/11/11/2127931/us...</td>\n",
       "      <td>Lisa Levin</td>\n",
       "      <td>2011-11-15 00:00:00</td>\n",
       "      <td>A</td>\n",
       "      <td>[stock.n.01, future.n.01, down.n.01, net_incom...</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>710</td>\n",
       "      <td>Financials, Industrials, and Basic Materials S...</td>\n",
       "      <td>https://www.benzinga.com/trading-ideas/long-id...</td>\n",
       "      <td>Benjamin Lee</td>\n",
       "      <td>2011-09-08 00:00:00</td>\n",
       "      <td>A</td>\n",
       "      <td>[basic.n.01, material.n.01, slide.n.01, stoppi...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>548</td>\n",
       "      <td>Earnings Expectations For The Week Of August 12</td>\n",
       "      <td>https://www.benzinga.com/news/earnings/13/08/3...</td>\n",
       "      <td>Nelson Hem</td>\n",
       "      <td>2013-08-11 00:00:00</td>\n",
       "      <td>A</td>\n",
       "      <td>[net_income.n.01, expectation.n.01, week.n.01,...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>243</td>\n",
       "      <td>Agilent Files Civil Lawsuit against Shanghai E...</td>\n",
       "      <td>https://www.benzinga.com/news/17/08/9964556/ag...</td>\n",
       "      <td>Charles Gross</td>\n",
       "      <td>2017-08-23 00:00:00</td>\n",
       "      <td>A</td>\n",
       "      <td>[file.n.01, lawsuit.n.01, shanghai.n.01, techn...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Export as csv for faster access\r\n",
    "data.to_csv('../data/analyst_ratings_processed_hl_cleaned.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Construct"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### API call to get StockPrices for all stocks that occur more than 10 times\r\n",
    "used API : 'https://polygon.io/'\r\n",
    "(need timeouts because of max 5 Api calls per Minute)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# List of Stock Ticker to call\r\n",
    "stocks_unique = unique(data['stock'])\r\n",
    "print(\"Es verbleiben \"+str(len(stocks_unique))+\" eindeutige Stocks, zur Anfrage an der API, welche im Zeitraum der Verfügbaren API liegen\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# API call with TimeOut (5 per minute)\r\n",
    "key = os.environ.get(\"POLYGON_IO_API_KEY\")\r\n",
    "apiResults = []\r\n",
    "apiCount = 0\r\n",
    "\r\n",
    "for s in stocks_unique:\r\n",
    "    # the API only response to 2years historical dates\r\n",
    "    from_ = '2019-08-21'\r\n",
    "    to =  '2020-12-31'\r\n",
    "\r\n",
    "    #The API call\r\n",
    "    with RESTClient(key) as client:\r\n",
    "        resp = client.stocks_equities_aggregates(s, 1, \"day\", from_, to, unadjusted=False)\r\n",
    "        # save the nessecary attributes of the JSON as List\r\n",
    "        if (resp and hasattr(resp, 'results')):\r\n",
    "            for result in resp.results:\r\n",
    "                dt = datetime.datetime.fromtimestamp(result['t'] / 1000.0).isoformat()\r\n",
    "                apiResults.append([s, dt, result['o'], result['h'], result['l'], result['c']])\r\n",
    "        else: \r\n",
    "            apiResults.append([s])\r\n",
    "            print(\"[\"+datetime.datetime.now().isoformat() +\"]\" + \" - API-Call Nr. \" + str(apiCount) + \" for Stock: \" + s + \" ___no_Results___\")\r\n",
    "    apiCount+=1\r\n",
    "    print(\"[\"+datetime.datetime.now().isoformat() +\"]\" + \" - API-Call Nr. \" + str(apiCount) + \" for Stock: \" + s + \" ___success___\")\r\n",
    "    time.sleep(12)\r\n",
    "\r\n",
    "#save the List-Data as DataFrame\r\n",
    "stock_prices = pd.DataFrame(apiResults, columns=['stock', 'date', 'open', 'high', 'low', 'close'])\r\n",
    "#save the DataFrame as csv\r\n",
    "stock_prices.to_csv('../data/raw_stock_prices.csv', encoding='utf-8', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#get the API data from the csv to DataFrame\r\n",
    "stock_prices = pd.read_csv('../data/raw_stock_prices.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Overview of the API Data\r\n",
    "\r\n",
    "Remove Stocks that has no Results from DataFrame Stock_prices and Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stock_prices.count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stock_prices.sample(7)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# count NaN rows\r\n",
    "nanStocks = stock_prices[stock_prices['open'].isna() & stock_prices['close'].isna()]['stock']\r\n",
    "print(\"Total Count of Stocks with NaN: \" + str(len(nanStocks)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Combine DataFrame\r\n",
    "\r\n",
    "open and close price of the Stock"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# remove headlines with NaN stock prices\r\n",
    "data = data.drop(data[data['stock'].isin(nanStocks)].index)\r\n",
    "\r\n",
    "# remove NaN stocks\r\n",
    "stock_prices = stock_prices.drop(stock_prices[stock_prices.open.isna() & stock_prices.close.isna()].index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# format datetime to date\r\n",
    "stock_prices = stock_prices.assign(\r\n",
    "    date = lambda dataframe: dataframe['date'].map(lambda date: date[0:10])\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# remove headlines with date that are not covered by the stockprices -> probably Weekend\r\n",
    "# datetime.datetime.fromisoformat().weekday() in [5,6]\r\n",
    "data = data.drop(data[~data['date'].isin(stock_prices['date'])].index) # ~ means NOT IN"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stock_prices.isna().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = pd.merge(data, stock_prices[['stock', 'date', 'open', 'close']], on=['stock', 'date'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Format"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Datetime Format already done for requesting the API and Merging the DateFrames<br>\r\n",
    "No more formating needed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Export Cleaned and Merged DataFrame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export final DataSet as csv\r\n",
    "data.to_csv('../data/analyst_ratings_processed_final.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import final DataSet for Modelling\r\n",
    "data = pd.read_csv('../data/analyst_ratings_processed_final.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Understanding: Visualization after Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Overview of the Final DataFrame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(data.columns)\r\n",
    "data.sample(7)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count_zeroSenti = data[data['senti_score'] == 0.000]['id'].count()\r\n",
    "count_noSenti = data[(data['senti_pos_score'] == 0.000) & (data['senti_neg_score'] == 0.000)]['id'].count()\r\n",
    "count_Senti = data[data['senti_score'] != 0.000]['id'].count()\r\n",
    "count_Senti_neg = data[data['senti_score'] < 0.000]['id'].count()\r\n",
    "count_Senti_pos = data[data['senti_score'] > 0.000]['id'].count()\r\n",
    "count_eachSenti = data[(data['senti_pos_score'] != 0.000) & (data['senti_neg_score'] != 0.000)]['id'].count()\r\n",
    "\r\n",
    "print(\"Im folgenden wird die Verteilung des Sentiment-Score über die Headlines genauer betrachtet: \\n%d headlines haben einen durchschnittlichen Sentiment von 0, davon haben %d weder einen positiven noch einen negativen Sentiment-Score. \\nHingegegen haben %d Headlines einen Sentiment-Score gesetzt der ungleich 0 ist. %d positive, %d negative. Dabei haben %d jeweils positive und negative zu bewertende Wörter.\" % (count_zeroSenti,count_noSenti,count_Senti,count_Senti_neg,count_Senti_pos,count_eachSenti))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"observations: {}, features: {} \\n\".format(data.shape[0], data.shape[1]))\r\n",
    "print(\"unique headlines: {}, unique stocks: {} \\n\".format(len(data.headline.unique()), len(data.stock.unique())))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data['stock'].value_counts().plot.box(vert=False, figsize=(30,10))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stock = data.groupby(\"stock\")\r\n",
    "# Number of Headlines by Stock\r\n",
    "plt.figure(figsize=(30,10))\r\n",
    "ax = stock.size().sort_values(ascending=False).plot.bar()\r\n",
    "plt.xticks(rotation=50)\r\n",
    "plt.xlabel(\"Stock\")\r\n",
    "plt.ylabel(\"Number of Headlines\")\r\n",
    "#only display every n Label\r\n",
    "n = 50\r\n",
    "for i, t in enumerate(ax.get_xticklabels()):\r\n",
    "    if (i % n) != 0:\r\n",
    "        t.set_visible(False)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Number of Headlines by Date\r\n",
    "date = data.groupby(\"date\")\r\n",
    "\r\n",
    "plt.figure(figsize=(30,10))\r\n",
    "ax = date.size().sort_values(ascending=False).plot.bar()\r\n",
    "plt.xticks(rotation=50)\r\n",
    "plt.xlabel(\"Date\")\r\n",
    "plt.ylabel(\"Number of Headlines\")\r\n",
    "\r\n",
    "#only display every n Label\r\n",
    "n = 50\r\n",
    "for i, t in enumerate(ax.get_xticklabels()):\r\n",
    "    if (i % n) != 0:\r\n",
    "        t.set_visible(False)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tempString = "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Termdocument Matrix\r\n",
    "\r\n",
    "# all headlines as String in List\r\n",
    "docs_afterCleaning = []\r\n",
    "for headline in data.headline_cleaned:\r\n",
    "    docs_afterCleaning.append(' '.join(list(map(lambda s: s.strip()[1:-5].replace('.',''), headline[1:-1].split(',')))))\r\n",
    "\r\n",
    "vec = CountVectorizer()\r\n",
    "X = vec.fit_transform(docs_afterCleaning)\r\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\r\n",
    "print(df)\r\n",
    "\r\n",
    "# !! TDM just useful for stemmend and removed Stopwords dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Wordclouds after Data Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Wordcloud for all Headlines\r\n",
    "\r\n",
    "#combine all headlines to one text\r\n",
    "text_afterCleaning = \" \"\r\n",
    "for headline in data.headline_cleaned:\r\n",
    "    text_afterCleaning = text_afterCleaning + ' '.join(list(map(lambda s: s.strip()[1:-5].replace('.',''), headline[1:-1].split(','))))\r\n",
    "\r\n",
    "maxWords = 50\r\n",
    "#wordcloud\r\n",
    "#No Stopword removal - Stopwords already removed\r\n",
    "# stopwords = STOPWORDS\r\n",
    "# stopwords.update([\"Benzinga\", \"Stocks\", \"vs\", \"Est\", \"EPS\"])\r\n",
    "wordcloud = WordCloud(max_words=maxWords).generate(text_afterCleaning)\r\n",
    "\r\n",
    "#plot\r\n",
    "print(\"WorldCloud over all Stocks, Top \" + str(maxWords) + \" Words:\")\r\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\r\n",
    "plt.axis(\"off\")\r\n",
    "plt.show()\r\n",
    "\r\n",
    "#save\r\n",
    "wordcloud.to_file(\"./results/wordcloud_afterCleaning.png\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Top n-Words\r\n",
    "n = 20\r\n",
    "\r\n",
    "wordFreq = WordCloud().process_text(text_afterCleaning)\r\n",
    "wordFreq = dict(sorted(wordFreq.items(), key=lambda item: item[1], reverse=True)[:n])\r\n",
    "\r\n",
    "plt.bar(range(len(wordFreq)), list(wordFreq.values()), align='center')\r\n",
    "plt.xticks(range(len(wordFreq)), list(wordFreq.keys()), rotation=50)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Topic Modelling after Data Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Topic Modelling with topicCount Topics\r\n",
    "topicCount = 10\r\n",
    "\r\n",
    "headline_toks = []\r\n",
    "for headline in data.headline_cleaned:\r\n",
    "    headline_toks.append(list(map(lambda s: s.strip()[1:-5].replace('.',''), headline[1:-1].split(','))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create Dictionary\r\n",
    "id2word = corpora.Dictionary(headline_toks)\r\n",
    "# Create Corpus\r\n",
    "texts = headline_toks\r\n",
    "# Term Document Frequency\r\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The Real Topic Modelling\r\n",
    "# Build LDA model\r\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\r\n",
    "                                       id2word=id2word,\r\n",
    "                                       num_topics=topicCount)\r\n",
    "# Print the Keyword in the topics\r\n",
    "pprint(lda_model.print_topics())\r\n",
    "doc_lda = lda_model[corpus]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualization of the Topic Modelling\r\n",
    "\r\n",
    "pyLDAvis.enable_notebook()\r\n",
    "LDAvis_data_filepath = os.path.join('./app/results_cleaned_ldavis_prepared_'+str(topicCount))\r\n",
    "# # this is a bit time consuming - make the if statement True\r\n",
    "# # if you want to execute visualization prep yourself\r\n",
    "if 1 == 1:\r\n",
    "    LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\r\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\r\n",
    "        pickle.dump(LDAvis_prepared, f)\r\n",
    "# load the pre-prepared pyLDAvis data from disk\r\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\r\n",
    "    LDAvis_prepared = pickle.load(f)\r\n",
    "pyLDAvis.save_html(LDAvis_prepared, './app/results_cleaned_ldavis_prepared_'+ str(topicCount) +'.html')\r\n",
    "LDAvis_prepared"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "e9f94fa94278d1b9bbc15332e8f5dc1ba941effd7ffdca1aa2632c253aefc311"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}