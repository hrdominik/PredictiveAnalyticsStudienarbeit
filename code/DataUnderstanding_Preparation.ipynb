{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Daily Financial News for 6000+ Stocks\r\n",
    "### Some Data Understanding an Preparation for Prediction of stockpricse by the Sentiment of Headlines\r\n",
    "@author DHR <br>\r\n",
    "@author BKN <br>\r\n",
    "used Data in this Notebook: 'https://www.kaggle.com/miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests'"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Requirements"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#immport basic DataScience Modules\r\n",
    "#!! for pip-modul-list to install @see requirements.txt\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#import basic topic modelling\r\n",
    "import re\r\n",
    "from gensim.utils import simple_preprocess\r\n",
    "import gensim.corpora as corpora\r\n",
    "from pprint import pprint\r\n",
    "import gensim\r\n",
    "\r\n",
    "import pyLDAvis\r\n",
    "import pyLDAvis.gensim_models\r\n",
    "import pickle "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#import basic visualization\r\n",
    "from wordcloud import WordCloud, STOPWORDS\r\n",
    "import nltk #tokenization\r\n",
    "#nltk.download('punkt')\r\n",
    "nltk.download('stopwords')\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#imports for generell Cleaning and Merging\r\n",
    "import datetime"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#imports headline cleaning\r\n",
    "import nltk\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "import re\r\n",
    "import spacy\r\n",
    "from nltk.corpus import sentiwordnet as swn\r\n",
    "from nltk.corpus import wordnet as wn\r\n",
    "nltk.download('wordnet')\r\n",
    "nltk.download('averaged_perceptron_tagger')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#import basic api requirements\r\n",
    "from polygon import RESTClient\r\n",
    "import time\r\n",
    "from dotenv import load_dotenv\r\n",
    "load_dotenv()\r\n",
    "\r\n",
    "def unique(l):\r\n",
    "    ## list of only the unique values from a given list\r\n",
    "    x = np.array(l)\r\n",
    "    return np.unique(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Load Data\r\n",
    "data = pd.read_csv('../data/raw_analyst_ratings.csv')\r\n",
    "#colums: id,headline,url,publisher,date,stock"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Understanding\r\n",
    "\r\n",
    "Without Data Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#first Look\r\n",
    "print(data.columns)\r\n",
    "data.sample(7)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#statistics\r\n",
    "print(\"observations: {}, features: {} \\n\".format(data.shape[0], data.shape[1]))\r\n",
    "print(\"unique headlines: {}, unique stocks: {} \\n\".format(len(data.headline.unique()), len(data.stock.unique())))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stock = data.groupby(\"stock\")\r\n",
    "stock.sample()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stock.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Visualization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# BoxPlot of Stocks\r\n",
    "data['stock'].value_counts().plot.box(vert=False, figsize=(30,10))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# BocPlot of Date\r\n",
    "data['date'].value_counts().plot.box(vert=False, figsize=(30,10))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Number of Headlines by Stock\r\n",
    "plt.figure(figsize=(30,10))\r\n",
    "ax = stock.size().sort_values(ascending=False)[0:6204].plot.bar()\r\n",
    "plt.xticks(rotation=50)\r\n",
    "plt.xlabel(\"Stock\")\r\n",
    "plt.ylabel(\"Number of Headlines\")\r\n",
    "#only display every n Label\r\n",
    "n = 50\r\n",
    "for i, t in enumerate(ax.get_xticklabels()):\r\n",
    "    if (i % n) != 0:\r\n",
    "        t.set_visible(False)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Number of Headlines by Date\r\n",
    "date = data.groupby(\"date\")\r\n",
    "\r\n",
    "plt.figure(figsize=(30,10))\r\n",
    "ax = date.size().sort_values(ascending=False)[0:1000].plot.bar()\r\n",
    "plt.xticks(rotation=50)\r\n",
    "plt.xlabel(\"Date\")\r\n",
    "plt.ylabel(\"Number of Headlines\")\r\n",
    "\r\n",
    "#only display every n Label\r\n",
    "n = 50\r\n",
    "for i, t in enumerate(ax.get_xticklabels()):\r\n",
    "    if (i % n) != 0:\r\n",
    "        t.set_visible(False)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "date_withYear = data.assign(year = lambda dataframe: dataframe['date'].map(lambda date: date[0:4]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Der Datensatz erstreckt sich über einen Datumsspanne vom \"+ data['date'].min()[0:10] +\" bis zum \"+ data['date'].max()[0:10] +\"\\nDabei besteht der Datensatz aus \"+ str(data.headline.count()) + \" Einträgen\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Number of Date by Year\r\n",
    "year = date_withYear.groupby(\"year\")\r\n",
    "\r\n",
    "plt.figure(figsize=(30,10))\r\n",
    "year.size().plot.bar()\r\n",
    "plt.xticks(rotation=50)\r\n",
    "plt.xlabel(\"Year\")\r\n",
    "plt.ylabel(\"Number of Healines\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import statistics\r\n",
    "\r\n",
    "print(statistics.mean(year.size()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Termdocument Matrix\r\n",
    "\r\n",
    "# all headlines as String in List\r\n",
    "docs = []\r\n",
    "for headline in data.headline:\r\n",
    "    docs.append(headline)\r\n",
    "docs = docs[0:1000]\r\n",
    "\r\n",
    "vec = CountVectorizer()\r\n",
    "X = vec.fit_transform(docs)\r\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\r\n",
    "print(df)\r\n",
    "\r\n",
    "# !! TDM just useful for stemmend and removed Stopwords dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Wordclouds before Data Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Wordcloud for all Headlines\r\n",
    "\r\n",
    "#combine all headlines to one text\r\n",
    "text = \" \".join(headline for headline in data.headline)\r\n",
    "maxWords = 50\r\n",
    "#wordcloud\r\n",
    "#!! No Stopword removal\r\n",
    "# stopwords = STOPWORDS\r\n",
    "# stopwords.update([\"Benzinga\", \"Stocks\", \"vs\", \"Est\", \"EPS\"])\r\n",
    "wordcloud = WordCloud(max_words=maxWords).generate(text)\r\n",
    "\r\n",
    "#plot\r\n",
    "print(\"WorldCloud over all Stocks, Top \" + str(maxWords) + \" Words:\")\r\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\r\n",
    "plt.axis(\"off\")\r\n",
    "plt.show()\r\n",
    "\r\n",
    "#save\r\n",
    "wordcloud.to_file(\"./results/wordcloud.png\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Wordcloud for one Stock\r\n",
    "\r\n",
    "#combine all headlines of one Stock\r\n",
    "stockFilter = random.choice(unique(data['stock']))\r\n",
    "maxWords = 50\r\n",
    "text_SpecificStock = \" \".join(headline for headline in data[data[\"stock\"]==stockFilter].headline)\r\n",
    "\r\n",
    "#wordcloud\r\n",
    "#!! No Stopword removal\r\n",
    "wordcloud = WordCloud(max_words=maxWords).generate(text_SpecificStock)\r\n",
    "\r\n",
    "#plot\r\n",
    "print(\"WorldCloud for Random Stock: \" + stockFilter + \", Top \" + str(maxWords) + \" Words:\")\r\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\r\n",
    "plt.axis(\"off\")\r\n",
    "plt.show()\r\n",
    "\r\n",
    "#save\r\n",
    "wordcloud.to_file(\"./results/wordcloud_stock-\"+ stockFilter +\".png\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Top n-Words\r\n",
    "n = 20\r\n",
    "\r\n",
    "wordFreq = WordCloud().process_text(text)\r\n",
    "wordFreq = dict(sorted(wordFreq.items(), key=lambda item: item[1], reverse=True)[:n])\r\n",
    "\r\n",
    "plt.bar(range(len(wordFreq)), list(wordFreq.values()), align='center')\r\n",
    "plt.xticks(range(len(wordFreq)), list(wordFreq.keys()), rotation=50)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Topic Modelling before Data Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Topic Modelling with topicCount Topics\r\n",
    "topicCount = 10\r\n",
    "\r\n",
    "## Necessary Text Cleaning\r\n",
    "# Remove punctuation\r\n",
    "# data['headline_processed'] = \\\r\n",
    "# data['headline'].map(lambda x: re.sub('[,\\.!?]', '', x))\r\n",
    "# Convert the titles to lowercase\r\n",
    "# data['headline_processed'] = \\\r\n",
    "# data['headline_processed'].map(lambda x: x.lower())\r\n",
    "\r\n",
    "# stop_words = stopwords.words('english')\r\n",
    "# stop_words.extend(['bezinga', 'stock'])\r\n",
    "\r\n",
    "## Necessary Tokenzisation of Sentences to Words\r\n",
    "def sent_to_words(sentences):\r\n",
    "    for sentence in sentences:\r\n",
    "        # deacc=True removes punctuations\r\n",
    "        yield(simple_preprocess(str(sentence), deacc=True))\r\n",
    "# def remove_stopwords(texts):\r\n",
    "#     return [[word for word in simple_preprocess(str(doc)) \r\n",
    "#              if word not in stop_words] for doc in texts]\r\n",
    "\r\n",
    "\r\n",
    "headline = data.headline.values.tolist() #use headline_processed instead of headline for a little bit of DataCleaning\r\n",
    "headline_words = list(sent_to_words(headline))\r\n",
    "# remove stop words\r\n",
    "# headline_words = remove_stopwords(headline_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create Dictionary\r\n",
    "id2word = corpora.Dictionary(headline_words)\r\n",
    "# Create Corpus\r\n",
    "texts = headline_words\r\n",
    "# Term Document Frequency\r\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The Real Topic Modelling\r\n",
    "# Build LDA model\r\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\r\n",
    "                                       id2word=id2word,\r\n",
    "                                       num_topics=topicCount)\r\n",
    "# Print the Keyword in the topics\r\n",
    "pprint(lda_model.print_topics())\r\n",
    "doc_lda = lda_model[corpus]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualization of the Topic Modelling\r\n",
    "\r\n",
    "pyLDAvis.enable_notebook()\r\n",
    "LDAvis_data_filepath = os.path.join('./app/results_ldavis_prepared_'+str(topicCount))\r\n",
    "# # this is a bit time consuming - make the if statement True\r\n",
    "# # if you want to execute visualization prep yourself\r\n",
    "if 1 == 1:\r\n",
    "    LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\r\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\r\n",
    "        pickle.dump(LDAvis_prepared, f)\r\n",
    "# load the pre-prepared pyLDAvis data from disk\r\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\r\n",
    "    LDAvis_prepared = pickle.load(f)\r\n",
    "pyLDAvis.save_html(LDAvis_prepared, './app/results_ldavis_prepared_'+ str(topicCount) +'.html')\r\n",
    "LDAvis_prepared"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Quality"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# count all null values in the DataFrame\r\n",
    "print(\"Anzahl von null Werten im gesamten DataFrame: \"+str(data.isna().sum().sum()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# count all dates without time, no need to format in isoFormat\r\n",
    "print(\"Anzahl Zeitstempel ohne konkrete Uhrzeit: \"+ str(data['date'].str.count('00:00:00').sum()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# count duplicated\r\n",
    "print(\"Anzahl Doppelten Einträgen: \"+ str(data.duplicated().sum()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Date Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# format datetime to date\r\n",
    "data = data.assign(\r\n",
    "    date = lambda dataframe: dataframe['date'].map(lambda date: date[0:10])\r\n",
    ")\r\n",
    "\r\n",
    "# delete all rows older than 2years, because of API and Performance\r\n",
    "data = data.drop(data[data.date < '2019-08-21'].index)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# clean the Stocks\r\n",
    "# Remove Rows with Stocks that occur less than occ times\r\n",
    "occ = 0\r\n",
    "data = data[data.groupby('stock').stock.transform(len) > occ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Der Datensatz erstreckt sich nun über einen Datumsspanne vom \"+ data['date'].min() +\" bis zum \"+ data['date'].max() +\"\\nDabei besteht der Datensatz jetzt aus \"+ str(data.headline.count()) + \" Einträgen\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Headline Cleaning & Pre-Processing\r\n",
    "Hier wird nochmal speziell jede einzelene Headline den typischen pre-processing Schritten unterzogen"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#daten nochmal neu laden falls reset\r\n",
    "data = pd.read_csv('../data/raw_analyst_ratings.csv')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#einen ausschnitt zum testen verwenden\r\n",
    "data = data[:1000]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#POS Tagging\r\n",
    "#Beim Part Of Speech Tagging wird geprüft, ob es sich um ein Nomen, Adjevtiv etc. handelt. Das unterstützt das Lammatisieren\r\n",
    "\r\n",
    "def tokenize_post(headline):\r\n",
    "    hl_tokenz = word_tokenize(headline)\r\n",
    "    hl_post = nltk.pos_tag(hl_tokenz)\r\n",
    "    hl_post_result = []\r\n",
    "    for word in hl_post:\r\n",
    "        if word[1].startswith('NN'):\r\n",
    "            hl_post_result.append([word[0], 'n'])\r\n",
    "        elif word[1].startswith('JJ'):\r\n",
    "            hl_post_result.append([word[0], 'a'])\r\n",
    "        elif word[1].startswith('V'):\r\n",
    "            hl_post_result.append([word[0], 'v'])\r\n",
    "        elif word[1].startswith('R'):\r\n",
    "            hl_post_result.append([word[0], 'r'])\r\n",
    "        else:\r\n",
    "            hl_post_result.append([word[0], ''])\r\n",
    "    return hl_post_result\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Tokenization und POS-Tagging durchführen\r\n",
    "data['headline_cleaned'] = data['headline'].apply(tokenize_post)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "CC coordinating conjunction\r\n",
    "CD cardinal digit\r\n",
    "DT determiner\r\n",
    "EX existential there (like: “there is” … think of it like “there exists”)\r\n",
    "FW foreign word\r\n",
    "IN preposition/subordinating conjunction\r\n",
    "JJ adjective ‘big’\r\n",
    "JJR adjective, comparative ‘bigger’\r\n",
    "JJS adjective, superlative ‘biggest’\r\n",
    "LS list marker 1)\r\n",
    "MD modal could, will\r\n",
    "NN noun, singular ‘desk’\r\n",
    "NNS noun plural ‘desks’\r\n",
    "NNP proper noun, singular ‘Harrison’\r\n",
    "NNPS proper noun, plural ‘Americans’\r\n",
    "PDT predeterminer ‘all the kids’\r\n",
    "POS possessive ending parent’s\r\n",
    "PRP personal pronoun I, he, she\r\n",
    "PRP$ possessive pronoun my, his, hers\r\n",
    "RB adverb very, silently,\r\n",
    "RBR adverb, comparative better\r\n",
    "RBS adverb, superlative best\r\n",
    "RP particle give up\r\n",
    "TO, to go ‘to’ the store.\r\n",
    "UH interjection, errrrrrrrm\r\n",
    "VB verb, base form take\r\n",
    "VBD verb, past tense took\r\n",
    "VBG verb, gerund/present participle taking\r\n",
    "VBN verb, past participle taken\r\n",
    "VBP verb, sing. present, non-3d take\r\n",
    "VBZ verb, 3rd person sing. present takes\r\n",
    "WDT wh-determiner which\r\n",
    "WP wh-pronoun who, what\r\n",
    "WP$ possessive wh-pronoun whose\r\n",
    "WRB wh-abverb where, when"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#funktionen für Lowercase und Stopword-removal\r\n",
    "\r\n",
    "#alle pos-tagged wörter der headlines werden jetzt nochmal kleingeschrieben\r\n",
    "def lowercase(headline_post_tok):\r\n",
    "    hl_post_tok_lower = []\r\n",
    "    for word_pos in headline_post_tok:\r\n",
    "        hl_post_tok_lower.append(tuple([word_pos[0].lower(), word_pos[1]]))\r\n",
    "    return hl_post_tok_lower\r\n",
    "    \r\n",
    "#Stopwords aus dem NLTK Modul nehmen und entfernen\r\n",
    "def remove_stopwords(hl_post_tok_lower):\r\n",
    "    stop_words = set(stopwords.words('english')) \r\n",
    "\r\n",
    "    #hier könnte man noch ein paar eigene Stopwords hinzufügen\r\n",
    "    # stop_words.add('')\r\n",
    "\r\n",
    "    filtered_sentence = [word for word in hl_post_tok_lower if not word[0] in stop_words]\r\n",
    "    return filtered_sentence"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Headlines lowercase machen und danach anzeigen\r\n",
    "data['headline_cleaned'] = data['headline_cleaned'].apply(lowercase)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#jetzt das stopword removal durchführen\r\n",
    "data['headline_cleaned'] = data['headline_cleaned'].apply(remove_stopwords)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Hier eine Lemmatisierungsfunktion mit dem WortNetLemmatizer\r\n",
    "lemmatizer = WordNetLemmatizer()\r\n",
    "\r\n",
    "#this headline is pos-tagged, tokenzied, lower, and stopwords-removed\r\n",
    "def lemmatize(headline):\r\n",
    "    lemmatized_output = []\r\n",
    "    for word in headline:\r\n",
    "        if word[1] == '':\r\n",
    "            lemmatized_output.append((lemmatizer.lemmatize(word[0]), word[1]))\r\n",
    "        else:\r\n",
    "            lemmatized_output.append((lemmatizer.lemmatize(word[0], pos=word[1]), word[1]))\r\n",
    "\r\n",
    "    #Alle Wörter mit weniger als zwei Zeichen weg\r\n",
    "    lemmatized_output = [word for word in lemmatized_output if len(word[0]) > 2]\r\n",
    "\r\n",
    "    #Alle Zahlen entfernen\r\n",
    "    lemmatized_output = [word for word in lemmatized_output if not word[0].isnumeric()]\r\n",
    "\r\n",
    "    return lemmatized_output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#lemmatization durchführen\r\n",
    "\r\n",
    "#nächste Zeile um daten zu laden\r\n",
    "# data_important = pd.read_csv('../data/zwischenergebnis_stopwords_tokenized.csv')\r\n",
    "data['headline_cleaned'] = data['headline_cleaned'].apply(lemmatize)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Mit ein Synset ist eine Liste von den gebräuchlisten Synonymen für ein Wort. zu jedem Wort nehmen wir das gebräuchliste Synonym, um möglichst viele Senitment Werte zu bekommen. \r\n",
    "def getSynset(headline):\r\n",
    "    synset_output = []\r\n",
    "    for word in headline:\r\n",
    "        synsets = wn.synsets(word[0], pos=word[1])\r\n",
    "        if len(synsets)>0:\r\n",
    "            synset_output.append((synsets[0].name()))\r\n",
    "        # wort entfernen wenn kein senitment verfügbar\r\n",
    "        # else:\r\n",
    "        #     synset_output.append(word[0]+ \".\" +word[1]+\".01\")\r\n",
    "    return synset_output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Synset berechnen\r\n",
    "data['headline_cleaned'] = data['headline_cleaned'].apply(getSynset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#calculate sentiment\r\n",
    "nltk.download('sentiwordnet')\r\n",
    "senti_pos_score = []\r\n",
    "senti_neg_score = []\r\n",
    "\r\n",
    "senti_score = []\r\n",
    "# senti_obj_score = []\r\n",
    "\r\n",
    "#wir sollten ein eignens Dictionary bauen, dass spezielle Wörter für Aktiennews beinhaltet, wie hit, raise etc. und\r\n",
    "def getSentiment(headline):\r\n",
    "    senti_pos = 0\r\n",
    "    senti_neg = 0\r\n",
    "    # senti_obj = 0\r\n",
    "    for word in headline:\r\n",
    "        #hier erst prüfen, ob das wort in unserem eigenen dictionary ist\r\n",
    "        swn_synset = swn.senti_synset(word)\r\n",
    "        senti_pos += swn_synset.pos_score()\r\n",
    "        senti_neg += swn_synset.neg_score()\r\n",
    "\r\n",
    "    senti_pos_score.append(senti_pos)\r\n",
    "    senti_neg_score.append(senti_neg)\r\n",
    "\r\n",
    "    senti_score.append(senti_pos - senti_neg)\r\n",
    "    # senti_obj_score.append(swn_synset.obj_score())\r\n",
    "    return headline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# calculate Sentiment\r\n",
    "data['headline_cleaned'] = data['headline_cleaned'].apply(getSentiment)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# add Sentiment to DataFrame\r\n",
    "data['senti_pos_score'] = senti_pos_score\r\n",
    "data['senti_neg_score'] = senti_neg_score\r\n",
    "\r\n",
    "data['senti_score'] = senti_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.sample(7)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Export as csv for faster access\r\n",
    "data.to_csv('../data/analyst_ratings_processed_hl_cleaned.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Construct"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### API call to get StockPrices for all stocks that occur more than 10 times\r\n",
    "used API : 'https://polygon.io/'\r\n",
    "(need timeouts because of max 5 Api calls per Minute)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# List of Stock Ticker to call\r\n",
    "stocks_unique = unique(data['stock'])\r\n",
    "print(\"Es verbleiben \"+str(len(stocks_unique))+\" eindeutige Stocks, zur Anfrage an der API, welche im Zeitraum der Verfügbaren API liegen\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# API call with TimeOut (5 per minute)\r\n",
    "key = os.environ.get(\"POLYGON_IO_API_KEY\")\r\n",
    "apiResults = []\r\n",
    "apiCount = 0\r\n",
    "\r\n",
    "for s in stocks_unique:\r\n",
    "    # the API only response to 2years historical dates\r\n",
    "    from_ = '2019-08-21'\r\n",
    "    to =  '2020-12-31'\r\n",
    "\r\n",
    "    #The API call\r\n",
    "    with RESTClient(key) as client:\r\n",
    "        resp = client.stocks_equities_aggregates(s, 1, \"day\", from_, to, unadjusted=False)\r\n",
    "        # save the nessecary attributes of the JSON as List\r\n",
    "        if (resp and hasattr(resp, 'results')):\r\n",
    "            for result in resp.results:\r\n",
    "                dt = datetime.datetime.fromtimestamp(result['t'] / 1000.0).isoformat()\r\n",
    "                apiResults.append([s, dt, result['o'], result['h'], result['l'], result['c']])\r\n",
    "        else: \r\n",
    "            apiResults.append([s])\r\n",
    "            print(\"[\"+datetime.datetime.now().isoformat() +\"]\" + \" - API-Call Nr. \" + str(apiCount) + \" for Stock: \" + s + \" ___no_Results___\")\r\n",
    "    apiCount+=1\r\n",
    "    print(\"[\"+datetime.datetime.now().isoformat() +\"]\" + \" - API-Call Nr. \" + str(apiCount) + \" for Stock: \" + s + \" ___success___\")\r\n",
    "    time.sleep(12)\r\n",
    "\r\n",
    "#save the List-Data as DataFrame\r\n",
    "stock_prices = pd.DataFrame(apiResults, columns=['stock', 'date', 'open', 'high', 'low', 'close'])\r\n",
    "#save the DataFrame as csv\r\n",
    "stock_prices.to_csv('../data/raw_stock_prices.csv', encoding='utf-8', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#get the API data from the csv to DataFrame\r\n",
    "stock_prices = pd.read_csv('../data/raw_stock_prices.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Overview of the API Data\r\n",
    "\r\n",
    "Remove Stocks that has no Results from DataFrame Stock_prices and Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stock_prices.count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stock_prices.sample(7)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# count NaN rows\r\n",
    "nanStocks = stock_prices[stock_prices['open'].isna() & stock_prices['close'].isna()]['stock']\r\n",
    "print(\"Total Count of Stocks with NaN: \" + str(len(nanStocks)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Combine DataFrame\r\n",
    "\r\n",
    "open and close price of the Stock"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# remove headlines with NaN stock prices\r\n",
    "data = data.drop(data[data['stock'].isin(nanStocks)].index)\r\n",
    "\r\n",
    "# remove NaN stocks\r\n",
    "stock_prices = stock_prices.drop(stock_prices[stock_prices.open.isna() & stock_prices.close.isna()].index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# format datetime to date\r\n",
    "stock_prices = stock_prices.assign(\r\n",
    "    date = lambda dataframe: dataframe['date'].map(lambda date: date[0:10])\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# remove headlines with date that are not covered by the stockprices -> probably Weekend\r\n",
    "# datetime.datetime.fromisoformat().weekday() in [5,6]\r\n",
    "data = data.drop(data[~data['date'].isin(stock_prices['date'])].index) # ~ means NOT IN"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stock_prices.isna().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = pd.merge(data, stock_prices[['stock', 'date', 'open', 'close']], on=['stock', 'date'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Format"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Datetime Format already done for requesting the API and Merging the DateFrames<br>\r\n",
    "No more formating needed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Export Cleaned and Merged DataFrame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export final DataSet as csv\r\n",
    "data.to_csv('../data/analyst_ratings_processed_final.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import final DataSet for Modelling\r\n",
    "data = pd.read_csv('../data/analyst_ratings_processed_final.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Understanding: Visualization after Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Overview of the Final DataFrame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(data.columns)\r\n",
    "data.sample(7)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count_zeroSenti = data[data['senti_score'] == 0.000]['id'].count()\r\n",
    "count_noSenti = data[(data['senti_pos_score'] == 0.000) & (data['senti_neg_score'] == 0.000)]['id'].count()\r\n",
    "count_Senti = data[data['senti_score'] != 0.000]['id'].count()\r\n",
    "count_Senti_neg = data[data['senti_score'] < 0.000]['id'].count()\r\n",
    "count_Senti_pos = data[data['senti_score'] > 0.000]['id'].count()\r\n",
    "count_eachSenti = data[(data['senti_pos_score'] != 0.000) & (data['senti_neg_score'] != 0.000)]['id'].count()\r\n",
    "\r\n",
    "print(\"Im folgenden wird die Verteilung des Sentiment-Score über die Headlines genauer betrachtet: \\n%d headlines haben einen durchschnittlichen Sentiment von 0, davon haben %d weder einen positiven noch einen negativen Sentiment-Score. \\nHingegegen haben %d Headlines einen Sentiment-Score gesetzt der ungleich 0 ist. %d positive, %d negative. Dabei haben %d jeweils positive und negative zu bewertende Wörter.\" % (count_zeroSenti,count_noSenti,count_Senti,count_Senti_neg,count_Senti_pos,count_eachSenti))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"observations: {}, features: {} \\n\".format(data.shape[0], data.shape[1]))\r\n",
    "print(\"unique headlines: {}, unique stocks: {} \\n\".format(len(data.headline.unique()), len(data.stock.unique())))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data['stock'].value_counts().plot.box(vert=False, figsize=(30,10))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stock = data.groupby(\"stock\")\r\n",
    "# Number of Headlines by Stock\r\n",
    "plt.figure(figsize=(30,10))\r\n",
    "ax = stock.size().sort_values(ascending=False).plot.bar()\r\n",
    "plt.xticks(rotation=50)\r\n",
    "plt.xlabel(\"Stock\")\r\n",
    "plt.ylabel(\"Number of Headlines\")\r\n",
    "#only display every n Label\r\n",
    "n = 50\r\n",
    "for i, t in enumerate(ax.get_xticklabels()):\r\n",
    "    if (i % n) != 0:\r\n",
    "        t.set_visible(False)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Number of Headlines by Date\r\n",
    "date = data.groupby(\"date\")\r\n",
    "\r\n",
    "plt.figure(figsize=(30,10))\r\n",
    "ax = date.size().sort_values(ascending=False).plot.bar()\r\n",
    "plt.xticks(rotation=50)\r\n",
    "plt.xlabel(\"Date\")\r\n",
    "plt.ylabel(\"Number of Headlines\")\r\n",
    "\r\n",
    "#only display every n Label\r\n",
    "n = 50\r\n",
    "for i, t in enumerate(ax.get_xticklabels()):\r\n",
    "    if (i % n) != 0:\r\n",
    "        t.set_visible(False)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tempString = "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Termdocument Matrix\r\n",
    "\r\n",
    "# all headlines as String in List\r\n",
    "docs_afterCleaning = []\r\n",
    "for headline in data.headline_cleaned:\r\n",
    "    docs_afterCleaning.append(' '.join(list(map(lambda s: s.strip()[1:-5].replace('.',''), headline[1:-1].split(',')))))\r\n",
    "\r\n",
    "vec = CountVectorizer()\r\n",
    "X = vec.fit_transform(docs_afterCleaning)\r\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\r\n",
    "print(df)\r\n",
    "\r\n",
    "# !! TDM just useful for stemmend and removed Stopwords dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Wordclouds after Data Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Wordcloud for all Headlines\r\n",
    "\r\n",
    "#combine all headlines to one text\r\n",
    "text_afterCleaning = \" \"\r\n",
    "for headline in data.headline_cleaned:\r\n",
    "    text_afterCleaning = text_afterCleaning + ' '.join(list(map(lambda s: s.strip()[1:-5].replace('.',''), headline[1:-1].split(','))))\r\n",
    "\r\n",
    "maxWords = 50\r\n",
    "#wordcloud\r\n",
    "#No Stopword removal - Stopwords already removed\r\n",
    "# stopwords = STOPWORDS\r\n",
    "# stopwords.update([\"Benzinga\", \"Stocks\", \"vs\", \"Est\", \"EPS\"])\r\n",
    "wordcloud = WordCloud(max_words=maxWords).generate(text_afterCleaning)\r\n",
    "\r\n",
    "#plot\r\n",
    "print(\"WorldCloud over all Stocks, Top \" + str(maxWords) + \" Words:\")\r\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\r\n",
    "plt.axis(\"off\")\r\n",
    "plt.show()\r\n",
    "\r\n",
    "#save\r\n",
    "wordcloud.to_file(\"./results/wordcloud_afterCleaning.png\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Top n-Words\r\n",
    "n = 20\r\n",
    "\r\n",
    "wordFreq = WordCloud().process_text(text_afterCleaning)\r\n",
    "wordFreq = dict(sorted(wordFreq.items(), key=lambda item: item[1], reverse=True)[:n])\r\n",
    "\r\n",
    "plt.bar(range(len(wordFreq)), list(wordFreq.values()), align='center')\r\n",
    "plt.xticks(range(len(wordFreq)), list(wordFreq.keys()), rotation=50)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Topic Modelling after Data Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Topic Modelling with topicCount Topics\r\n",
    "topicCount = 10\r\n",
    "\r\n",
    "headline_toks = []\r\n",
    "for headline in data.headline_cleaned:\r\n",
    "    headline_toks.append(list(map(lambda s: s.strip()[1:-5].replace('.',''), headline[1:-1].split(','))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create Dictionary\r\n",
    "id2word = corpora.Dictionary(headline_toks)\r\n",
    "# Create Corpus\r\n",
    "texts = headline_toks\r\n",
    "# Term Document Frequency\r\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The Real Topic Modelling\r\n",
    "# Build LDA model\r\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\r\n",
    "                                       id2word=id2word,\r\n",
    "                                       num_topics=topicCount)\r\n",
    "# Print the Keyword in the topics\r\n",
    "pprint(lda_model.print_topics())\r\n",
    "doc_lda = lda_model[corpus]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualization of the Topic Modelling\r\n",
    "\r\n",
    "pyLDAvis.enable_notebook()\r\n",
    "LDAvis_data_filepath = os.path.join('./app/results_cleaned_ldavis_prepared_'+str(topicCount))\r\n",
    "# # this is a bit time consuming - make the if statement True\r\n",
    "# # if you want to execute visualization prep yourself\r\n",
    "if 1 == 1:\r\n",
    "    LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\r\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\r\n",
    "        pickle.dump(LDAvis_prepared, f)\r\n",
    "# load the pre-prepared pyLDAvis data from disk\r\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\r\n",
    "    LDAvis_prepared = pickle.load(f)\r\n",
    "pyLDAvis.save_html(LDAvis_prepared, './app/results_cleaned_ldavis_prepared_'+ str(topicCount) +'.html')\r\n",
    "LDAvis_prepared"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "e9f94fa94278d1b9bbc15332e8f5dc1ba941effd7ffdca1aa2632c253aefc311"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}